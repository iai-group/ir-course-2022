{"cells":[{"cell_type":"markdown","metadata":{"id":"WsEvNqgAzZVU"},"source":["# Experiments with text classifiers in sklearn"]},{"cell_type":"markdown","metadata":{"id":"5wkrFjwWzZVX"},"source":["In this exercise we'll be experimenting with various classification algorithms in scikit learn using the [20 Newsgroups collection](http://people.csail.mit.edu/jrennie/20Newsgroups/).\n","\n","The first part of the notebook shows a detailed example usage of text classification using sklearn (based on [scikit learn's \"Working with text data\" tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)).\n","The real exercise is at the bottom, where you'll be asked to perform various experiments."]},{"cell_type":"markdown","metadata":{"id":"s719rR66zZVX"},"source":["## Load data"]},{"cell_type":"markdown","metadata":{"id":"wTCwWnNmzZVY"},"source":["In order to get faster execution times, we will work on a partial dataset with only 5 categories out of the 20 available in the dataset:"]},{"cell_type":"code","source":["pip install ipytest"],"metadata":{"id":"ghak5ZNgzzO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6GeX5yjzZVY"},"outputs":[],"source":["categories = [\n","    \"alt.atheism\",\n","    \"soc.religion.christian\", \n","    \"talk.religion.misc\",\n","    \"comp.sys.ibm.pc.hardware\",\n","    \"comp.sys.mac.hardware\"\n","]"]},{"cell_type":"markdown","metadata":{"id":"bMpzLSitzZVZ"},"source":["We load the documents from those categories, divided into train and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScYYuK6IzZVa"},"outputs":[],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","train = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=123)\n","test = fetch_20newsgroups(subset=\"test\", categories=categories, shuffle=True, random_state=123)"]},{"cell_type":"markdown","metadata":{"id":"btKGbZKjzZVa"},"source":["Check which categories got loaded."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8w87HviRzZVb"},"outputs":[],"source":["print(train.target_names)"]},{"cell_type":"markdown","metadata":{"id":"oWQ_L3iJzZVc"},"source":["Check the size of training and test splits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjZiT0h9zZVc"},"outputs":[],"source":["print(\"Training instances: {}\".format(len(train.data)))\n","print(\"Test instances:     {}\".format(len(test.data)))"]},{"cell_type":"markdown","metadata":{"id":"s4hxcU99zZVc"},"source":["Check target labels of some of the train and test instances."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJiHcb4izZVc"},"outputs":[],"source":["print(train.target[:10])\n","print(test.target[:10])"]},{"cell_type":"markdown","metadata":{"id":"OdEI8_vtzZVd"},"source":["## Train a model"]},{"cell_type":"markdown","metadata":{"id":"mcml1_YVzZVd"},"source":["Bag-of-words document representation, using raw term counts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7boAI3LzZVd"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","count_vect = CountVectorizer()\n","X_train_counts = count_vect.fit_transform(train.data)"]},{"cell_type":"markdown","metadata":{"id":"xREGZRI2zZVd"},"source":["Check dimensionality (instances x features)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoZHOQmfzZVd"},"outputs":[],"source":["print(X_train_counts.shape)"]},{"cell_type":"markdown","metadata":{"id":"DWJWOTC3zZVe"},"source":["Check vocabulary (sample 10 terms)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DB9DFnYVzZVe"},"outputs":[],"source":["for idx, term in enumerate(count_vect.vocabulary_.keys()):\n","    if idx < 10:\n","        print(f\"{term} (ID: {count_vect.vocabulary_[term]})\")"]},{"cell_type":"markdown","metadata":{"id":"oX8q_ieIzZVe"},"source":["Learn a Naive Bayes model on the training data (by default it uses Laplace smoothing with alpha=1)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibU3WR94zZVe"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","classifier = MultinomialNB(alpha=1.0)\n","classifier.fit(X_train_counts, train.target)"]},{"cell_type":"markdown","metadata":{"id":"tkcxksxZzZVe"},"source":["## Apply the model"]},{"cell_type":"markdown","metadata":{"id":"qWnv7pMIzZVf"},"source":["First, extract the same feature representation by re-using the `CountVectorizer` from before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKOUTIHszZVf"},"outputs":[],"source":["X_test_counts = count_vect.transform(test.data)"]},{"cell_type":"markdown","metadata":{"id":"YaRzekVHzZVf"},"source":["Check dimensionality (documents x features)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrZ1ggT8zZVf"},"outputs":[],"source":["print(X_test_counts.shape)"]},{"cell_type":"markdown","metadata":{"id":"EidAzsHgzZVf"},"source":["Then, predict labels for test instances."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGH8oqLBzZVf"},"outputs":[],"source":["predicted = classifier.predict(X_test_counts)"]},{"cell_type":"markdown","metadata":{"id":"B78fcNfgzZVf"},"source":["Look at some of the predicted labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSnJFC89zZVf"},"outputs":[],"source":["print(predicted[:10])"]},{"cell_type":"markdown","metadata":{"id":"QkbnrOltzZVf"},"source":["## Evaluate model performance\n","\n","We use Accuracy as our measure here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pA4Bl2WHzZVf"},"outputs":[],"source":["from sklearn import metrics\n","\n","print(f\"{metrics.accuracy_score(test.target, predicted):.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"0ZgiZ86OzZVg"},"source":["## Exercise\n","\n","1) Use TF weighting instead of the raw counts. (See the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) for `TfidfTransformer` usage.)\n","\n","2) Try at least one different classifier, e.g., [linear SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) (or [other SVMs](https://scikit-learn.org/stable/modules/svm.html#svm-classification)).\n","\n","3) Record the results you got in the table below. How far can you push accuracy?"]},{"cell_type":"markdown","metadata":{"id":"_JlwGaW8zZVg"},"source":["### Solution"]},{"cell_type":"markdown","metadata":{"id":"339UoHUmzZVg"},"source":["Building a pipeline for each row in the table, then running an evaluating them in a single loop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NkvMIEWzZVg"},"outputs":[],"source":["from sklearn.linear_model import SGDClassifier\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.pipeline import Pipeline"]},{"cell_type":"markdown","metadata":{"id":"gTtr7_1ZzZVg"},"source":["#### Naive Bayes variants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEzogpa6zZVg"},"outputs":[],"source":["pipeline_nb_raw = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', MultinomialNB()),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKX64ELSzZVg"},"outputs":[],"source":["pipeline_nb_tf = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf', TfidfTransformer(use_idf=False)),\n","    ('clf', MultinomialNB()),\n","])"]},{"cell_type":"markdown","metadata":{"id":"wunbIC4szZVg"},"source":["#### SVM variants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmuKZfL6zZVg"},"outputs":[],"source":["pipeline_svm_raw = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', SGDClassifier()),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v39uMKY6zZVh"},"outputs":[],"source":["pipeline_svm_tf = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf', TfidfTransformer(use_idf=False)),\n","    ('clf', SGDClassifier()),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AeBm0nzzZVh"},"outputs":[],"source":["for pipeline in [\n","    pipeline_nb_raw, pipeline_nb_tf, \n","    pipeline_svm_raw, pipeline_svm_tf\n","]:\n","    pipeline.fit(train.data, train.target)\n","    predicted = pipeline.predict(test.data)\n","    print(f\"{metrics.accuracy_score(test.target, predicted):.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"iRkFu7JgzZVm"},"source":["### Results\n","\n","| Model | Term weighting | Accuracy |\n","| -- | -- |:--:|\n","| Naive Bayes | Raw counts | 0.864 |\n","| Naive Bayes | TF | 0.667 |\n","| SVM | Raw counts | 0.819 |\n","| SVM | TF | 0.851 |\n","| ... | ... | ... | \n"]},{"cell_type":"markdown","metadata":{"id":"nk7QEVvLzZVm"},"source":["## Optional exercise\n","\n","Can you push performance ever further? You could try, for example, more sophisticated text preprocessing (tokenization, stopwords removal, and stemming) using [NLTK](https://www.nltk.org/) (which is part of the Anaconda distribution). See, e.g., [this article](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a) for some hints."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"E2-3-Text_classification_sklearn.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}