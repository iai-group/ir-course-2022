{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Entity linking evaluation\n","You are provided with the documents annotations along with ground truth annotations and asked to evaluate them."],"metadata":{"id":"ibWxbNBhIXOh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AuO8T7LUIONA"},"outputs":[],"source":["pip install ipytest"]},{"cell_type":"code","source":["import ipytest\n","import pytest\n","\n","ipytest.autoconfig()"],"metadata":{"id":"qbd9N7s5IiLM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The annotations given by a entity linking system under evaluation."],"metadata":{"id":"8nu_VkIVgjnv"}},{"cell_type":"code","source":["LINKED_ENTITIES_1 = [ \n","    (0, 'angola', 'wikipedia:Angola'),\n","    (14, 'multiparty democracy', 'wikipedia:multiparty_democracy'),\n","    (18, '1992 elections', 'wikipedia:Philippine_general_election,_1992')\n","]\n","\n","LINKED_ENTITIES_2 = [\n","    (5, 'angola', 'wikipedia:Angola'),\n","    (10, '1975', 'wikipedia:Philippine_general_election,_1992'),\n","    (13, 'one party', 'wikipedia:Single-party_state')\n","]"],"metadata":{"id":"KL6yvSIcOkUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ground truth annotations (reference annotations)."],"metadata":{"id":"gHVEsm3hgsCQ"}},{"cell_type":"code","source":["GROUND_TRUTH_ANNOTATIONS_1 = [ \n","    (0, 'angola', 'wikipedia:Angola'),\n","    (4, 'one-party', 'wikipedia:Single-party_state'),\n","    (14, 'multiparty democracy', 'wikipedia:multiparty_democracy'),\n","    (18, '1992 elections', 'wikipedia:Philippine_general_election,_1992')\n","]\n","\n","GROUND_TRUTH_ANNOTATIONS_2 = [\n","    (5, 'angola', 'wikipedia:Angola'),\n","    (13, 'one party', 'wikipedia:Single-party_state'),\n","    (14, 'Republic', 'wikipedia:Republic')\n","]"],"metadata":{"id":"4q_5Ij3fQWR0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set-based metrics where:\n","- precision is defined as the fraction of correctly linked entities that have been annotated by the system\n","- recall is defined as fraction of correctly linked entities that should be annotated \n","- F-measure is a harmonic mean between precision and recall"],"metadata":{"id":"DOsTzvL_g1e7"}},{"cell_type":"code","source":["def set_based_precision(annotations, relevance_annotations):\n","  \"\"\"Computes set-based precision.\n","  \n","  Args:\n","      annotations: All annotations for a set of documents.\n","      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n","      \n","  Returns:\n","      Set-based precision.    \n","  \"\"\"\n","  return len(set(annotations).intersection(relevance_annotations))/len(annotations)"],"metadata":{"id":"ZMDYNJVZX6Hs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_based_recall(annotations, relevance_annotations):\n","  \"\"\"Computes set-based recall.\n","  \n","  Args:\n","      annotations: All annotations for a set of documents.\n","      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n","      \n","  Returns:\n","      Set-based recall.    \n","  \"\"\"\n","  return len(set(annotations).intersection(relevance_annotations))/len(relevance_annotations)"],"metadata":{"id":"YHITcKHIYK6r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def f1_score(precision, recall):\n","  \"\"\"Computes F-measure.\n","  \n","  Args:\n","      annotations: All annotations for a set of documents.\n","      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n","      \n","  Returns:\n","      F-measure.    \n","  \"\"\"\n","  return 2 * precision * recall / (precision + recall)"],"metadata":{"id":"qWRO31XBdtcy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Metrics over the collection of documents\n","\n","Micro-averaged - averaged across mentions"],"metadata":{"id":"tcBzUihbhb_j"}},{"cell_type":"code","source":["import itertools \n","\n","def micro_precision(annotations, ground_truth_annotations):\n","  \"\"\"Computes micro-averaged precision.\n","  \n","  Args:\n","      annotations: All annotations for a set of documents.\n","      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n","      \n","  Returns:\n","      Micro-averaged precision.    \n","  \"\"\"\n","  all_annotations = list(itertools.chain(*annotations))\n","  all_ground_truth_annotations = list(itertools.chain(*ground_truth_annotations))\n","  return set_based_precision(all_annotations, all_ground_truth_annotations)"],"metadata":{"id":"hpSNh02zQ29O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def micro_recall(all_annotations, ground_truth_annotations):\n","  \"\"\"Computes micro-averaged recall.\n","  \n","  Args:\n","      annotations: All annotations for a set of documents.\n","      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n","      \n","  Returns:\n","      Micro-averaged recall.    \n","  \"\"\"\n","  all_annotations = list(itertools.chain(*all_annotations))\n","  all_ground_truth_annotations = list(itertools.chain(*ground_truth_annotations))\n","  return set_based_recall(all_annotations, all_ground_truth_annotations)"],"metadata":{"id":"KoKdOBzhSta7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tests"],"metadata":{"id":"Np0FExe9TSpH"}},{"cell_type":"code","source":["%%run_pytest[clean]\n","\n","def test_micro_precision():\n","  assert micro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx(5/6, rel=1e-2)\n","\n","def test_micro_recall():\n","  assert micro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx(5/7, rel=1e-2)\n","\n","def test_micro_f1():\n","  micro_p = micro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n","  micro_r = micro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n","  assert f1_score(micro_p, micro_r) == pytest.approx((2 * 5/6 * 5/7) / (5/6 + 5/7), rel=1e-2)"],"metadata":{"id":"f78_TzYnTSUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Macro-averaged - averaged across documents"],"metadata":{"id":"6vNQN5oIhl7x"}},{"cell_type":"code","source":["def macro_precision(annotations, ground_truth_annotations):\n","  \"\"\"Computes macro-averaged precision.\n","  \n","  Args:\n","      annotations: All annotations for a set of documents.\n","      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n","      \n","  Returns:\n","      Macro-averaged precision.    \n","  \"\"\"\n","  return sum(set_based_precision(annotation, ground_truth) for annotation, ground_truth \n","             in zip(annotations, ground_truth_annotations))/len(ground_truth_annotations)"],"metadata":{"id":"0lYkqs2dXG0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def macro_recall(annotations, ground_truth_annotations):\n","  \"\"\"Computes macro-averaged recall.\n","  \n","  Args:\n","      annotations: All annotations for a set of documents.\n","      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n","      \n","  Returns:\n","      Macro-averaged recall.    \n","  \"\"\"\n","  return sum(set_based_recall(annotation, ground_truth) for annotation, ground_truth \n","             in zip(annotations, ground_truth_annotations))/len(ground_truth_annotations)"],"metadata":{"id":"KyXXm-_1YuKO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tests"],"metadata":{"id":"YCkmcF6_hqP1"}},{"cell_type":"code","source":["%%run_pytest[clean]\n","\n","def test_macro_precision():\n","  assert macro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx((1 + 2/3)/2, rel=1e-2)\n","\n","def test_macro_recall():\n","  assert macro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx((3/4 + 2/3)/2, rel=1e-2)\n","\n","def test_macro_f1():\n","  macro_p = macro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n","  macro_r = macro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n","  assert f1_score(macro_p, macro_r) == pytest.approx((2 * 5/6 * 17/24) / (5/6 + 17/24), rel=1e-2)"],"metadata":{"id":"rymX_XcYYy5n"},"execution_count":null,"outputs":[]}]}