{"cells":[{"cell_type":"markdown","metadata":{"id":"5gtEalrR37oa"},"source":["# NDCG Calculation"]},{"cell_type":"markdown","metadata":{"id":"aHmo-hyD37oc"},"source":["In this exercise, you'll have to evaluate system rankings, by computing the Normalized Discounted Cumulative Gain (NDCG) measure."]},{"cell_type":"code","source":["pip install ipytest"],"metadata":{"id":"QkCf9_gk3_is"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6Vxxoj837od"},"outputs":[],"source":["import ipytest\n","import math\n","import pytest\n","\n","from typing import Dict, List\n","\n","ipytest.autoconfig()"]},{"cell_type":"markdown","metadata":{"id":"fbRq-Nl037oe"},"source":["### Rankings produced for each query\n","\n","The key is the query ID (string), the value is a list of document IDs (ints)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjPhihZe37oe"},"outputs":[],"source":["system_rankings = {\n","    \"q1\": [2, 1, 3, 4, 5, 6, 10, 7, 9, 8],\n","    \"q2\": [1, 2, 9, 4, 5, 6, 7, 8, 3, 10],\n","    \"q3\": [1, 7, 4, 5, 3, 6, 9, 8, 10, 2]\n","}"]},{"cell_type":"markdown","metadata":{"id":"ATFATI3M37oe"},"source":["### Ground truth\n","\n","The key is the query ID, the value is a dictionary with (document ID, relevance) pairs. Relevance is measured on a 3-point scale: non-relevant (0), poor (1), good (2), excellent (3). Documents not listed here are non-relevant (relevance=0)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_GGNJ7h37oe"},"outputs":[],"source":["ground_truth = {\n","    \"q1\": {4: 3, 1: 2, 2: 1},\n","    \"q2\": {3: 3, 4: 3, 1: 2, 2: 1, 8: 1},\n","    \"q3\": {1: 3, 4: 3, 7: 2, 5: 2, 6: 1, 8: 1}\n","}"]},{"cell_type":"markdown","metadata":{"id":"xd917lkb37of"},"source":["## Computing evaluation metrics\n","\n","Discounted cumulative gain at rank $k$ is computed as:\n","\n","$$DCG_k = rel_1 + \\sum_{i=2}^k\\frac{rel_i}{\\log_2 i}$$\n","\n","Normalized discounted cumulative gain at rank $k$ is computed as:\n","\n","$$NDCG_k = \\frac{DCG_k}{IDCG_k}$$\n","\n","where $IDCG_k$ is the $DCG_k$ score of an idealized (perfect) ranking."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9qGPph637og"},"outputs":[],"source":["def dcg(relevances: List[int], k: int) -> float:\n","    \"\"\"Computes DCG@k, given the corresponding relevance levels for a ranked list of documents.\n","    \n","    For example, given a ranking [2, 3, 1] where the relevance levels according to the ground \n","    truth are {1:3, 2:4, 3:1}, the input list will be [4, 1, 3].\n","    \n","    Args:\n","        relevances: List with the ground truth relevance levels corresponding to a ranked list of documents.\n","        k: Rank cut-off.\n","        \n","    Returns:\n","        DCG@k (float).\n","    \"\"\"\n","    # Note: Rank position is indexed from 1.\n","    return relevances[0] + sum(\n","        rel / math.log(i + 2, 2) \n","         for i, rel in enumerate(relevances[1:k])\n","    )"]},{"cell_type":"code","source":["%%run_pytest[clean]\n","\n","@pytest.mark.parametrize(\"relevances,k,correct_value\", [\n","    ([4, 1, 3], 2, 5.0),\n","    ([4, 1, 3], 5, 6.893)\n","])\n","def test_dcg(relevances, k, correct_value):    \n","    assert dcg(relevances, k) == pytest.approx(correct_value, rel=1e-3)"],"metadata":{"id":"t_ZtaVPKi34z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tYOEa9Vh37og"},"outputs":[],"source":["def ndcg(system_ranking: List[int], ground_truth: List[int], k:int = 10) -> float:\n","    \"\"\"Computes NDCG@k for a given system ranking.\n","    \n","    Args:\n","        system_ranking: Ranked list of document IDs (from most to least relevant).\n","        ground_truth: Dict with document ID: relevance level pairs. Document not present here are to be taken with relevance = 0.\n","        k: Rank cut-off.\n","    \n","    Returns:\n","        NDCG@k (float).\n","    \"\"\"\n","    # Relevance levels for the ranked docs.\n","    relevances = [ground_truth.get(rank,0) for rank in system_ranking]\n","\n","    # Relevance levels of the idealized ranking.\n","    relevances_ideal = sorted(ground_truth.values(), reverse=True)\n","    \n","    return dcg(relevances, k) / dcg(relevances_ideal, k)        "]},{"cell_type":"markdown","metadata":{"id":"CAtpsZ0d37oh"},"source":["Tests."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5HeF2SZ37oh"},"outputs":[],"source":["%%run_pytest[clean]\n","\n","@pytest.mark.parametrize(\"qid,k,correct_value\", [\n","    (\"q1\", 5, 0.799),\n","    (\"q1\", 10, 0.799),\n","    (\"q2\", 5, 0.549),\n","    (\"q2\", 10, 0.705),\n","    (\"q3\", 5, 0.908),\n","    (\"q3\", 10, 0.949),\n","])\n","def test_queries(qid, k, correct_value):    \n","    assert ndcg(system_rankings[qid], ground_truth[qid], k) == pytest.approx(correct_value, rel=1e-3)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}