{"cells":[{"cell_type":"markdown","source":["# Exploring Word2Vec with Gensim"],"metadata":{"id":"8erErqJlrN8V"}},{"cell_type":"markdown","source":["## Overview"],"metadata":{"id":"P9FEOaWsrN8X"}},{"cell_type":"markdown","source":["Word2Vec is an approach to learning *word embeddings*, vector representations of words that capture semantic and syntactic relationships between words based on their co-occurrences in natural language text. \n","\n","This unsupervised learning approach also reduces the dimensionality of the vectors representing words, which can be helpful for memory and to manage the *curse of dimensionality*, whereby high-dimensional vector spaces lead to a relative data sparsity, e.g., for machine learning. "],"metadata":{"id":"km-s_ja2rN8X"}},{"cell_type":"markdown","source":["In this exercise you will look at the capabilities of Word2Vec as implemented in the module Gensim. "],"metadata":{"id":"uDcuhEiBrN8X"}},{"cell_type":"markdown","source":["## Requirements "],"metadata":{"id":"pW5GlR2PrN8Y"}},{"cell_type":"markdown","source":["Uncomment the lines below, run the installations once as needed, then comment the code out again."],"metadata":{"id":"JHwayhworN8Y"}},{"cell_type":"code","source":["pip install ipytest"],"metadata":{"id":"iE3Ev6HLr9kL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"source":["# !pip install --upgrade pip\n","# !pip install --upgrade Cython\n","# !pip install --upgrade gensim"],"outputs":[],"metadata":{"id":"f7Pgt9EorN8Y"}},{"cell_type":"markdown","source":["Import all necessary libraries. "],"metadata":{"id":"-x1hXqJArN8Z"}},{"cell_type":"code","execution_count":null,"source":["# Import modules and set up logging.\n","from typing import List, Generator\n","import gensim.downloader as api\n","from gensim.models import Word2Vec\n","import logging\n","import numpy as np\n","import os\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","import ipytest\n","import pytest\n","\n","ipytest.autoconfig()"],"outputs":[],"metadata":{"id":"2AnrOe_QrN8Z"}},{"cell_type":"markdown","source":["## Download data"],"metadata":{"id":"P60Uvf8yrN8Z"}},{"cell_type":"code","execution_count":null,"source":["# Load the Text8 corpus.\n","print(api.info('text8'))\n","text8_corpus = api.load('text8')"],"outputs":[{"output_type":"stream","name":"stdout","text":["{'num_records': 1701, 'record_format': 'list of str (tokens)', 'file_size': 33182058, 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py', 'license': 'not found', 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.', 'checksum': '68799af40b6bda07dfa47a32612e5364', 'file_name': 'text8.gz', 'read_more': ['http://mattmahoney.net/dc/textdata.html'], 'parts': 1}\n","[==================================================] 100.0% 31.6/31.6MB downloaded\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7i26KlerN8a","executionInfo":{"status":"ok","timestamp":1664303121657,"user_tz":-120,"elapsed":28524,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"46ef7716-a91f-4a93-f8f6-0068faa6e14e"}},{"cell_type":"markdown","source":["## Train a model"],"metadata":{"id":"WHWVODH5rN8a"}},{"cell_type":"code","execution_count":null,"source":["# Train a Word2Vec model on the Text8 corpus with default hyperparameters. \n","model = Word2Vec(text8_corpus)  \n","\n","# Perform a sanity check on the trained model.\n","print(model.wv.similarity('tree', 'leaf')) "],"outputs":[{"output_type":"stream","name":"stdout","text":["0.68252116\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kWbcFsrUrN8a","executionInfo":{"status":"ok","timestamp":1664303302874,"user_tz":-120,"elapsed":181229,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"540fe028-06de-4bb5-e9d2-3529c171a364"}},{"cell_type":"code","execution_count":null,"source":["# Reduce logging level.\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)"],"outputs":[],"metadata":{"id":"5RS3ddp3rN8a"}},{"cell_type":"code","execution_count":null,"source":["print(model.wv.most_similar('tree')) \n","print(model.wv.most_similar('leaf')) "],"outputs":[{"output_type":"stream","name":"stdout","text":["[('leaf', 0.6825212240219116), ('trees', 0.6816262006759644), ('bark', 0.6624914407730103), ('avl', 0.6079550981521606), ('fruit', 0.6029431819915771), ('flower', 0.6008095741271973), ('cactus', 0.6005957126617432), ('grass', 0.5823603868484497), ('bird', 0.5789984464645386), ('pond', 0.5772603750228882)]\n","[('bark', 0.7890558838844299), ('pigment', 0.7656276822090149), ('colored', 0.7648026347160339), ('reddish', 0.7575005888938904), ('coloured', 0.7569463849067688), ('grass', 0.7538576126098633), ('yellowish', 0.752110481262207), ('beetle', 0.7462701797485352), ('aloe', 0.7384286522865295), ('fleshy', 0.73822021484375)]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rHDVIKlFrN8a","executionInfo":{"status":"ok","timestamp":1664303302874,"user_tz":-120,"elapsed":22,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"70cd12a3-6ede-4894-b4fa-ff9c96a46c29"}},{"cell_type":"markdown","source":["## Relationships\n","\n","Investigate the relationships between words in terms of trained representations. "],"metadata":{"id":"4JfXVHjPrN8a"}},{"cell_type":"markdown","source":["### Evaluate  analogies\n","With the model you have trained, evaluate the analogy\n","`king-man+woman =~ queen`"],"metadata":{"id":"fvVb4RZ6rN8a"}},{"cell_type":"code","execution_count":null,"source":["print(model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))"],"outputs":[{"output_type":"stream","name":"stdout","text":["[('queen', 0.6803272366523743), ('empress', 0.6377588510513306), ('daughter', 0.6349191665649414), ('emperor', 0.6257573366165161), ('prince', 0.6172590255737305)]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMq0rNp6rN8b","executionInfo":{"status":"ok","timestamp":1664303302875,"user_tz":-120,"elapsed":21,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"69e69bca-9c44-400c-dfb8-953590b4a4d2"}},{"cell_type":"markdown","source":["Evaluate the analogy `ship-boat+rocket =~ spacecraft`. How similar are the left-hand side of the analogy to the right-hand side? Implement a function that can find the answer for analogies in general. We assume the right-hand side of the analogy will always be a single, positive term. "],"metadata":{"id":"jNPZWXZArN8b"}},{"cell_type":"code","execution_count":null,"source":["def eval_analogy(model: Word2Vec, lhs_pos: List[str], lhs_neg: List[str], rhs: str)->float:\n","    \"\"\"Returns the similarity between the left-hand and right-hand sides of an anaology.\n","    \n","        Arguments: \n","            model: Trained Gensim word2vec model to use.\n","            lhs_pos: List of terms that are positive on the left-hand side in the analogy. \n","            lhs_neg: List of terms that are negative on the left-hand side in the analogy. \n","            rhs: A single positive term on the right-hand side in the analogy.\n","            \n","        Returns:\n","            Float of the similarity if right-hand side term is found in the top 500 most similar terms.\n","            Otherwise, return None.\"\"\"\n","    # How similar are the left-hand side of the analogy to the right-hand side? \n","    # Implement a function that can find the answer for analogies in general.\n","    # TODO: Complete.\n","    similarities_list = model.most_similar(positive=lhs_pos, negative=lhs_neg, topn=500)\n","    similarities_dict = {}\n","    for term, sim in similarities_list:\n","        similarities_dict[term] = sim\n","    if rhs in similarities_dict:\n","        return similarities_dict[rhs]\n","    else:\n","        print(\"Right-hand side term not found in top 500 most similar terms to the left-hand side analogy.\")\n","        None"],"outputs":[],"metadata":{"id":"4Zv5s2AjrN8b"}},{"cell_type":"markdown","source":["Test:"],"metadata":{"id":"y47NEZeJrN8b"}},{"cell_type":"code","execution_count":null,"source":["%%run_pytest[clean]\n","\n","def test_eval_analogy():\n","    assert eval_analogy(model.wv, ['ship', 'rocket'], ['boat'], 'spacecraft') == pytest.approx(0.7, abs=1e-1)"],"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m"]},{"output_type":"stream","name":"stderr","text":["%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"]},{"output_type":"stream","name":"stdout","text":["\n","\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxbmLvSjrN8c","executionInfo":{"status":"ok","timestamp":1664303303219,"user_tz":-120,"elapsed":362,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"85623ee1-952c-4f1c-a19c-1fa2c4b48a28"}},{"cell_type":"markdown","source":["## Load a pre-trained model"],"metadata":{"id":"y3T6E2a8rN8c"}},{"cell_type":"code","execution_count":null,"source":["import gensim.downloader as api\n","model_loaded = api.load('word2vec-google-news-300')"],"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4pknof_arN8c","executionInfo":{"status":"ok","timestamp":1664304128237,"user_tz":-120,"elapsed":825020,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"22f8fa38-9fee-446b-cb64-1cc107ed3e20"}},{"cell_type":"code","execution_count":null,"source":["loaded_analogy_eval = -1\n","# Evaluate the analogy 'king'-'man'+'woman' compared to 'queen' using the loaded model \n","# and assign the value to the variable `loaded_analogy_eval`.\n","# TODO: Complete.\n","loaded_analogy_eval = eval_analogy(model_loaded, ['king', 'woman'], ['man'], 'queen')"],"outputs":[],"metadata":{"id":"zQFYufuxrN8c"}},{"cell_type":"code","execution_count":null,"source":["%%run_pytest[clean]\n","\n","def test_loaded_analogy_eval():\n","    assert loaded_analogy_eval != -1\n","    assert loaded_analogy_eval == pytest.approx(0.7, abs=1e-1)"],"outputs":[{"output_type":"stream","name":"stderr","text":["%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n","\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1bW9LVSrN8c","executionInfo":{"status":"ok","timestamp":1664304141901,"user_tz":-120,"elapsed":422,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"b6b7d5b6-bd0b-4f10-a44e-c46be9c10e4d"}},{"cell_type":"markdown","source":["## Train Word2Vec on different corpora"],"metadata":{"id":"SrPRdsZdrN8c"}},{"cell_type":"code","execution_count":null,"source":["# Download the rap lyrics of Kanye West.\n","! wget https://raw.githubusercontent.com/gsurma/text_predictor/master/data/kanye/input.txt\n","! mv input.txt kanye.txt\n","\n","# Download the complete works of William Shakespeare.\n","! wget https://raw.githubusercontent.com/gsurma/text_predictor/master/data/shakespeare/input.txt\n","! mv input.txt shakespeare.txt"],"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-09-27 18:42:21--  https://raw.githubusercontent.com/gsurma/text_predictor/master/data/kanye/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 330453 (323K) [text/plain]\n","Saving to: ‘input.txt’\n","\n","input.txt           100%[===================>] 322.71K  --.-KB/s    in 0.03s   \n","\n","2022-09-27 18:42:22 (9.42 MB/s) - ‘input.txt’ saved [330453/330453]\n","\n","--2022-09-27 18:42:22--  https://raw.githubusercontent.com/gsurma/text_predictor/master/data/shakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4573338 (4.4M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","input.txt           100%[===================>]   4.36M  --.-KB/s    in 0.08s   \n","\n","2022-09-27 18:42:23 (58.1 MB/s) - ‘input.txt’ saved [4573338/4573338]\n","\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hC7wUGL6rN8c","executionInfo":{"status":"ok","timestamp":1664304144110,"user_tz":-120,"elapsed":2212,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"80cab086-e335-4811-816e-13b4852798de"}},{"cell_type":"code","execution_count":null,"source":["from gensim.test.utils import datapath\n","from gensim import utils\n","\n","class MyCorpus:\n","    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n","    def __init__(self, data: str) -> None:\n","        self.data = data\n","\n","    def __iter__(self) -> Generator[List[str], None, None]:\n","        corpus_path = datapath(self.data)\n","        for line in open(corpus_path):\n","            # assume there's one document per line, tokens separated by whitespace\n","            yield utils.simple_preprocess(line)"],"outputs":[],"metadata":{"id":"_hUkv9ZirN8d"}},{"cell_type":"markdown","source":["Separately train two new models using the two different datasets, and compare how these datasets affect relationships between "],"metadata":{"id":"sOK7dp0vrN8d"}},{"cell_type":"code","execution_count":null,"source":["kanye_data = MyCorpus(os.getcwd()+'/kanye.txt')\n","shakespeare_data = MyCorpus(os.getcwd()+'/shakespeare.txt')"],"outputs":[],"metadata":{"id":"syhlDRCXrN8d"}},{"cell_type":"code","execution_count":null,"source":["kanye_model = None\n","# Train a Word2Vec model on the Kanye corpus, and name it `kanye_model`.\n","# TODO: Complete\n","kanye_model = Word2Vec(sentences=kanye_data)"],"outputs":[],"metadata":{"id":"SSZUSqyHrN8d"}},{"cell_type":"code","execution_count":null,"source":["shakespeare_model = None\n","# Train a Word2Vec model on the Shakespeare corpus, and name it `shakespeare_model`.\n","# TODO: Complete\n","shakespeare_model = Word2Vec(sentences=shakespeare_data)"],"outputs":[],"metadata":{"id":"xN4oiKairN8d"}},{"cell_type":"markdown","source":["For each of the models, we can easily find words where the two models learn very different similarities."],"metadata":{"id":"kvvBEVm1rN8d"}},{"cell_type":"code","execution_count":null,"source":["# For example, compare:\n","print(kanye_model.wv.most_similar(positive=['king'], topn=5))\n","print(shakespeare_model.wv.most_similar(positive=['king'], topn=5))"],"outputs":[{"output_type":"stream","name":"stdout","text":["[('big', 0.9997344017028809), ('his', 0.9997167587280273), ('by', 0.9996941089630127), ('might', 0.9996739625930786), ('ooh', 0.9996687769889832)]\n","[('prince', 0.8771281838417053), ('duke', 0.7768160104751587), ('fifth', 0.6678986549377441), ('gaunt', 0.664115846157074), ('queen', 0.6587375402450562)]\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yaa2hWkrN8d","executionInfo":{"status":"ok","timestamp":1664304162990,"user_tz":-120,"elapsed":9,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"acdaa12f-6ac9-4168-ac33-1edac0aa7959"}},{"cell_type":"markdown","source":["## Compare Skip-gram and CBOW\n","\n","By using the arguments of the model (training) method in `gensim.models.Word2Vec()` you can select either Skip-gram or CBOW explicitly, as well as modifying other hyperparameters. \n","\n","Train a Skip-gram model on the Text8 corpus and compare with the default CBOW model on the same dataset, with the same context window size, and compare how relationships are expressed in terms of the resulting embedding vectors.\n","\n","**Hint:** Use the keyword argument `sg` in when instantiating the model object to specify Skip-gram, rather than the defaul CBOW setting."],"metadata":{"id":"ixw2F4sDrN8d"}},{"cell_type":"code","execution_count":null,"source":["model_sg = None\n","# Train a skip-gram Word2Vec model on `text8_corpus` and name it `model_sg``\n","# TODO: Complete\n","model_sg = Word2Vec(text8_corpus, sg=1)"],"outputs":[],"metadata":{"id":"Udul-W-xrN8d"}},{"cell_type":"code","execution_count":null,"source":["loaded_analogy_eval_sg = eval_analogy(model_sg, ['king', 'woman'], ['man'], 'queen')"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  app.launch_new_instance()\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5nLuj7QrN8d","executionInfo":{"status":"ok","timestamp":1664304689450,"user_tz":-120,"elapsed":24,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"10ded2a8-fe17-4fbd-8f15-7bb9ec41ba63"}},{"cell_type":"code","execution_count":null,"source":["loaded_analogy_eval_cbow = eval_analogy(model, ['king', 'woman'], ['man'], 'queen')"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  app.launch_new_instance()\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7bmyAoyrN8e","executionInfo":{"status":"ok","timestamp":1664304689450,"user_tz":-120,"elapsed":19,"user":{"displayName":"Weronika Łajewska","userId":"08250876141368247860"}},"outputId":"7051dc39-0abe-434c-ed10-398db4c35089"}},{"cell_type":"markdown","source":["**Discuss:** Which of the models produces the highest similarity for the example analogy? Will this always be the case? Why or why not?"],"metadata":{"id":"LqS5gCcmrN8e"}},{"cell_type":"markdown","source":["For more information about Gensim, see https://radimrehurek.com/gensim."],"metadata":{"id":"1C8JgtObrN8e"}}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.9.7 64-bit ('dat640': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"interpreter":{"hash":"4f36a626e24aa200704e7a1da8e159d79437a6ae015274136a84a715398481c5"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}